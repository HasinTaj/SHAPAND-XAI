# -*- coding: utf-8 -*-
"""SHAP AND XAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IGKBdR3q5O3txaq7Dsc2klngZcinAb6r
"""

from google.colab import drive
drive.mount('/content/drive')


!pip install transformers datasets shap bertviz captum -q

import pandas as pd

# Path provided by you
path = "/content/drive/MyDrive/MBZUAI DATASET/archive (3)/Medical/0000.txt"
df = pd.read_csv(path)

# Display the first few rows to identify column names
print(df.head())

from transformers import pipeline

model_name = "distilbert-base-uncased-finetuned-sst-2-english"

pipe = pipeline("text-classification", model=model_name, top_k=None, device=0)

print(df.columns.tolist())

import pandas as pd


path = "/content/drive/MyDrive/MBZUAI DATASET/archive (3)/Medical/0000.txt"


df = pd.read_csv(path, header=None, names=['text', 'category'], encoding='utf-8')


print("--- New Column Names ---")
print(df.columns.tolist())
print("\n--- First Row Example ---")
print(df['text'].iloc[0][:100], "...")

import shap
from transformers import pipeline

# Re-initializing the explainer to fix your NameError
# Ensure 'pipe' was defined previously
model_name = "distilbert-base-uncased-finetuned-sst-2-english" # Update if using AraBERT
pipe = pipeline("text-classification", model=model_name, device=0, truncation=True)

# Define the explainer properly
explainer = shap.Explainer(pipe)

print("âœ… 'explainer' is now defined and ready!")

import shap
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

# 1. Load the specific model & tokenizer
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# 2. Create the pipeline with truncation FORCED
pipe = pipeline(
    "text-classification",
    model=model,
    tokenizer=tokenizer,
    device=0,
    truncation=True,
    max_length=512
)

# 3. Create a Custom Predict Function for SHAP
# This function manually cuts the text to 512 tokens to avoid the indexing error
def truncated_predictor(texts):
    # Truncate strings manually before they hit the tokenizer internally
    truncated_texts = [tokenizer.decode(tokenizer.encode(t, truncation=True, max_length=512), skip_special_tokens=True) for t in texts]
    return pipe(truncated_texts)

# 4. Initialize the SHAP explainer using the custom predictor
explainer = shap.Explainer(truncated_predictor, tokenizer)

print("âœ… Tokenizer, Pipeline, and Explainer are all ready!")

import shap
import numpy as np
import torch
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification

# 1. Setup Model
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# 2. Optimized Predictor Function
def predictor(texts):
    # This ensures every string is truncated before the model sees it
    inputs = tokenizer(texts.tolist(), return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.logits.softmax(dim=-1).numpy()

# 3. Use the Model-Agnostic Explainer (More stable than Partition for News)
masker = shap.maskers.Text(tokenizer)
explainer = shap.Explainer(predictor, masker)

print("âœ… System Ready. Errors rectified.")

def finalize_and_evaluate(text_index=0):
    text = df['text'].iloc[text_index]

    # 1. Get original prediction
    # Truncate manually for the single pipe call
    short_text = tokenizer.decode(tokenizer.encode(text, truncation=True, max_length=512), skip_special_tokens=True)
    orig_probs = predictor(np.array([short_text]))[0]
    label = np.argmax(orig_probs)

    # 2. Generate SHAP values (limiting evals for speed)
    shap_values = explainer([short_text], max_evals=100)

    # 3. Find Top 3 Words
    # Get indices of highest SHAP values for the predicted class
    vals = shap_values.values[0][:, label]
    top_indices = np.argsort(vals)[-3:]
    important_tokens = [shap_values.data[0][i] for i in top_indices]

    # 4. Masking (Faithfulness Test)
    masked_text = short_text
    for word in important_tokens:
        masked_text = masked_text.replace(word, "[MASK]")

    # 5. New Prediction
    new_probs = predictor(np.array([masked_text]))[0]
    drop = orig_probs[label] - new_probs[label]

    print(f"--- MBZUAI Project Results ---")
    print(f"Detected Important Words: {important_tokens}")
    print(f"Model Confidence Drop: {drop:.4f}")

    # Render the SHAP plot
    shap.plots.text(shap_values[0, :, label])
    return drop

# Run it!
finalize_and_evaluate(0)

import numpy as np

def run_batch_evaluation(dataframe, num_samples=10):
    all_drops = []
    print(f"ðŸš€ Starting Batch Evaluation on {num_samples} samples...")

    for i in range(num_samples):
        try:
            # Reusing your existing finalize_and_evaluate logic in a loop
            text = dataframe['text'].iloc[i]

            # 1. Original Prediction
            short_text = tokenizer.decode(tokenizer.encode(text, truncation=True, max_length=512), skip_special_tokens=True)
            orig_probs = predictor(np.array([short_text]))[0]
            label = np.argmax(orig_probs)

            # 2. Get SHAP Values
            shap_values = explainer([short_text], max_evals=100)
            vals = shap_values.values[0][:, label]
            top_indices = np.argsort(vals)[-3:]
            important_tokens = [shap_values.data[0][idx] for idx in top_indices]

            # 3. Masking
            masked_text = short_text
            for word in important_tokens:
                masked_text = masked_text.replace(word, "[MASK]")

            # 4. New Prediction
            new_probs = predictor(np.array([masked_text]))[0]
            drop = orig_probs[label] - new_probs[label]
            all_drops.append(drop)

            print(f"Sample {i+1}: Drop = {drop:.4f}")
        except Exception as e:
            print(f"Skipping sample {i+1} due to error: {e}")

    avg_drop = np.mean(all_drops)
    print(f"\nâœ… Evaluation Complete!")
    print(f"ðŸ“Š Average Faithfulness Score (Confidence Drop): {avg_drop:.4f}")
    return avg_drop

# Run on your dataset
avg_faithfulness = run_batch_evaluation(df, num_samples=10)

def mbzuai_demo(input_text):
    # Ensure text is treated as words, not chars
    short_text = tokenizer.decode(tokenizer.encode(input_text, truncation=True, max_length=512), skip_special_tokens=True)
    probs = predictor(np.array([short_text]))[0]

    # SHAP logic - Ensure it uses the word-level masker
    shap_values = explainer([short_text], max_evals=100)

    # Get the predicted label index
    label_idx = np.argmax(probs)
    # Correcting the label logic for your report
    label_name = "Medical/Health" if label_idx == 1 else "Non-Medical/General"

    # FIX: Ensure we get words, not single characters
    vals = shap_values.values[0][:, label_idx]
    top_idx = np.argsort(vals)[-3:]
    top_words = [shap_values.data[0][i].strip() for i in top_idx if len(shap_values.data[0][i].strip()) > 1]

    return {
        "Predicted Category": label_name,
        "Confidence Score": f"{probs[label_idx]:.2%}",
        "Top Influential Keywords": ", ".join(top_words) if top_words else "Analyzing..."
    }

!pip install gradio -q
import gradio as gr

def mbzuai_demo(input_text):
    # Truncate and Predict
    short_text = tokenizer.decode(tokenizer.encode(input_text, truncation=True, max_length=512), skip_special_tokens=True)
    probs = predictor(np.array([short_text]))[0]
    label_idx = np.argmax(probs)
    label_name = "Positive/Medical" if label_idx == 1 else "Negative/General"

    # Get SHAP Explanation
    shap_values = explainer([short_text], max_evals=100)
    vals = shap_values.values[0][:, label_idx]
    top_idx = np.argsort(vals)[-3:]
    top_words = [shap_values.data[0][i] for i in top_idx]

    return {
        "Predicted Category": label_name,
        "Confidence Score": f"{probs[label_idx]:.2%}",
        "Top Influential Keywords": ", ".join(top_words)
    }

# Create Interface
interface = gr.Interface(
    fn=mbzuai_demo,
    inputs=gr.Textbox(lines=5, label="Enter Arabic Text"),
    outputs="json",
    title="MBZUAI XAI Project: Transformer Interpretability",
    description="This dashboard uses SHAP to explain why a Transformer model classified a specific text."
)

interface.launch(share=True)